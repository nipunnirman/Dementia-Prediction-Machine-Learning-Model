{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "pd.set_option('display.max_columns', None)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/engineered_data.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify and Separate Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Target: DEMENTED\n",
      "   Distribution: {0: 137606, 1: 57590}\n"
     ]
    }
   ],
   "source": [
    "TARGET_CANDIDATES = ['DEMENTED', 'NORMCOG', 'DEMENTIA_BINARY', 'NACCALZD']\n",
    "\n",
    "TARGET = None\n",
    "for candidate in TARGET_CANDIDATES:\n",
    "    if candidate in df.columns:\n",
    "        TARGET = candidate\n",
    "        break\n",
    "\n",
    "if TARGET is None:\n",
    "    raise ValueError(\"No target variable found!\")\n",
    "\n",
    "print(f\" Target: {TARGET}\")\n",
    "print(f\"   Distribution: {df[TARGET].value_counts().to_dict()}\")\n",
    "\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "\n",
    "valid_idx = y.notna()\n",
    "X = X[valid_idx]\n",
    "y = y[valid_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Handle Non-Numeric Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: 58\n",
      "Non-numeric columns: 0\n",
      " All columns are numeric\n",
      "\n",
      "Final feature count: 58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Non-numeric columns: {len(non_numeric_cols)}\")\n",
    "\n",
    "if non_numeric_cols:\n",
    "    print(f\"\\n Encoding {len(non_numeric_cols)} columns with LabelEncoder...\")\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    for col in non_numeric_cols:\n",
    "        print(f\"   - Encoding {col}...\", end=\" \")\n",
    "        \n",
    "        # Convert to string and fill NaN\n",
    "        X[col] = X[col].astype(str).fillna('missing')\n",
    "        \n",
    "        # Label encode\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "        \n",
    "        print(\"✓\")\n",
    "    \n",
    "    print(f\"\\n    All columns encoded!\")\n",
    "    print(f\"   New shape: {X.shape}\")\n",
    "else:\n",
    "    print(\" All columns are numeric\")\n",
    "\n",
    "print(f\"\\nFinal feature count: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0 (0.00%)\n",
      " No missing values!\n",
      "\n",
      " Checking for infinite values...\n",
      "    No infinite values\n"
     ]
    }
   ],
   "source": [
    "missing_count = X.isnull().sum().sum()\n",
    "missing_pct = (missing_count / (X.shape[0] * X.shape[1])) * 100\n",
    "\n",
    "print(f\"Missing values: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"\\nTop 10 columns with missing data:\")\n",
    "    missing_by_col = X.isnull().sum().sort_values(ascending=False)\n",
    "    print(missing_by_col[missing_by_col > 0].head(10))\n",
    "    \n",
    "    # Impute\n",
    "    print(f\"\\n⚙️  Imputing with median strategy...\")\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    X = pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "    \n",
    "    print(f\"    Imputation complete!\")\n",
    "    print(f\"   Remaining missing: {X.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\" No missing values!\")\n",
    "\n",
    "# Handle infinite values\n",
    "print(f\"\\n Checking for infinite values...\")\n",
    "inf_count = np.isinf(X).sum().sum()\n",
    "if inf_count > 0:\n",
    "    print(f\"   ⚠️  Found {inf_count} infinite values, replacing with NaN...\")\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    # Re-impute\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    X = pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "    print(f\"   Handled infinite values\")\n",
    "else:\n",
    "    print(f\"    No infinite values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data split complete!\n",
      "\n",
      "Training set:\n",
      "   X_train: 156,156 rows x 58 features\n",
      "   y_train: 156,156 samples\n",
      "   Distribution: {0: 110084, 1: 46072}\n",
      "\n",
      "Test set:\n",
      "   X_test: 39,040 rows x 58 features\n",
      "   y_test: 39,040 samples\n",
      "   Distribution: {0: 27522, 1: 11518}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\" Data split complete!\\n\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"   X_train: {X_train.shape[0]:,} rows x {X_train.shape[1]:,} features\")\n",
    "print(f\"   y_train: {len(y_train):,} samples\")\n",
    "print(f\"   Distribution: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"   X_test: {X_test.shape[0]:,} rows x {X_test.shape[1]:,} features\")\n",
    "print(f\"   y_test: {len(y_test):,} samples\")\n",
    "print(f\"   Distribution: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling CRASH-PROOF VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Large dataset detected (156,156 rows)\n",
      "   Using sample to prevent kernel crash...\n",
      "    Reduced to 10,000 rows\n",
      "    NOTE: Using sample for training!\n",
      "\n",
      "Scaling 58 features...\n",
      "\n",
      " Scaling successful!\n",
      "   Train scaled: (10000, 58)\n",
      "   Test scaled: (39040, 58)\n",
      "\n",
      "Verification (first 3 features):\n",
      "   NACCID: mean=-0.0000, std=1.0001\n",
      "   NACCADC: mean=0.0000, std=1.0001\n",
      "   PACKET: mean=0.0000, std=1.0001\n"
     ]
    }
   ],
   "source": [
    "memory_mb = X_train.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "\n",
    "# CRITICAL: Use sample if dataset is large (prevents crashes)\n",
    "if len(X_train) > 10000:\n",
    "    print(f\"\\n Large dataset detected ({len(X_train):,} rows)\")\n",
    "    print(f\"   Using sample to prevent kernel crash...\")\n",
    "    \n",
    "    sample_size = min(10000, len(X_train))\n",
    "    sample_idx = X_train.sample(n=sample_size, random_state=RANDOM_STATE).index\n",
    "    \n",
    "    X_train = X_train.loc[sample_idx]\n",
    "    y_train = y_train.loc[sample_idx]\n",
    "    \n",
    "    print(f\"    Reduced to {len(X_train):,} rows\")\n",
    "    print(f\"    NOTE: Using sample for training!\")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "try:\n",
    "    print(f\"\\nScaling {X_train.shape[1]} features...\")\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        X_train_scaled,\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    \n",
    "    # Transform test data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        X_test_scaled,\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    print(f\"\\n Scaling successful!\")\n",
    "    print(f\"   Train scaled: {X_train_scaled.shape}\")\n",
    "    print(f\"   Test scaled: {X_test_scaled.shape}\")\n",
    "    \n",
    "    # Verify scaling\n",
    "    print(f\"\\nVerification (first 3 features):\")\n",
    "    for col in X_train_scaled.columns[:3]:\n",
    "        mean = X_train_scaled[col].mean()\n",
    "        std = X_train_scaled[col].std()\n",
    "        print(f\"   {col}: mean={mean:.4f}, std={std:.4f}\")\n",
    "\n",
    "except MemoryError:\n",
    "    print(\"\\n MEMORY ERROR OCCURRED!\")\n",
    "    print(\"\\n   Solutions:\")\n",
    "    print(\"   1. Reduce sample_size to 5000\")\n",
    "    print(\"   2. Restart kernel: Kernel → Restart & Clear Output\")\n",
    "    print(\"   3. Close other applications\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved: X_train_scaled.pkl\n",
      "   Saved: X_test_scaled.pkl\n",
      "    Saved: y_train.pkl\n",
      "   Saved: y_test.pkl\n",
      "    Saved: scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "with open('data/X_train_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_scaled, f)\n",
    "print(\"    Saved: X_train_scaled.pkl\")\n",
    "\n",
    "with open('data/X_test_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test_scaled, f)\n",
    "print(\"   Saved: X_test_scaled.pkl\")\n",
    "\n",
    "with open('data/y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "print(\"    Saved: y_train.pkl\")\n",
    "\n",
    "with open('data/y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "print(\"   Saved: y_test.pkl\")\n",
    "\n",
    "with open('data/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"    Saved: scaler.pkl\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
